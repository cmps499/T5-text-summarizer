{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**IMPORTS**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\nimport re\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\n\n# Plotting tools\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nn = 1000","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install  tensorflow-gpu==1.15.0","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip cased_L-12_H-768_A-12.zip","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nfrom bert import modeling\nimport tensorflow as tf\nimport numpy as np\nimport itertools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLTK Stop words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['background', 'methods', 'introduction', 'conclusions', 'results', \n                   'purpose', 'materials', 'discussions','methodology','result analysis'])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"list of files given for [COVID-19 Open Research Dataset challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) ","metadata":{}},{"cell_type":"code","source":"os.listdir('../input/CORD-19-research-challenge/')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* read the readme file first","metadata":{}},{"cell_type":"code","source":"with open('../input/CORD-19-research-challenge/metadata.readme', 'r') as f:\n    data = f.read()\n    print(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"biorxiv_dir = '/kaggle/input/CORD-19-research-challenge/document_parses/pdf_json'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The cell below shows that the updated datasets are in CSV format now - biorxiv_clean.csv, \n# clean_comm_use.csv, clean_noncomm_use.csv and clean_pmc.csv\nos.listdir( '/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading The updated clean CSV files**\n### Show the first 2 rows of each sub CSV files of the dataset bu using name of csv file.head(2)","metadata":{}},{"cell_type":"code","source":"biorxiv_clean = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv')\nclean_comm_use = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv')\nclean_noncomm_use =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv')\nclean_pmc =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"biorxiv_clean.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_comm_use.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_noncomm_use.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_pmc.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"first text of biorxiv_clean dataset","metadata":{}},{"cell_type":"code","source":"biorxiv_clean.text[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 :  Working with biorxiv","metadata":{}},{"cell_type":"markdown","source":"# biorxiv_clean papers Abstract - frequent words (400 sample)","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=30, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_wordcloud(biorxiv_clean['abstract'], title = 'biorxiv_clean - papers Abstract - frequent words (400 sample)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert abstract to list","metadata":{}},{"cell_type":"code","source":"df = biorxiv_clean\ndf = df.abstract.dropna() \ndata = df.values.tolist()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dropna() will delete rows that have missing value in the abstract column, tolist() converts the dataframe into a list.","metadata":{}},{"cell_type":"markdown","source":"* all the sentences and corresponding similar sentences have been stored in a pandas dataframe called simsentence**","metadata":{}},{"cell_type":"markdown","source":"### use the new dataframe as csv file for possible further research","metadata":{}},{"cell_type":"code","source":"simsentence = pd.read_csv('../input/simsentence/simsentence.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The function below converts sentences to words using gensim**","metadata":{}},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"yield will return a generator, that used gensim.utils.simple_preprocess() function and for each sentence, will convert it into list of preprocessed words","metadata":{}},{"cell_type":"markdown","source":"# Build the bigram and trigram models using gensim","metadata":{}},{"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=20) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=20)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[1]]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"creating a bigram model using the Phrases class from Gensim, using data_words which contains preprocessed word lists. The min_count parameter specifies the minimum frequency a phrase needs to appear in the corpus to be considered as a bigram, and the threshold parameter controls the threshold for forming the bigrams.\n\nThe Phraser class is a faster way to apply a trained phrase model to a sequence.\n","metadata":{}},{"cell_type":"markdown","source":"# Define functions for stopwords, bigrams, trigrams and lemmatization","metadata":{}},{"cell_type":"code","source":"#https://github.com/cjriggio/classifying_medical_innovation\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"text processing and lemmatization.\n\nremove_stopwords(texts), make_bigrams(texts), make_trigrams(texts) all are use a list comprehension to iterate over each word in texts lists. \n\nthen, performs lemmatization on a list of texts using SpaCy. It takes the texts list as input and Within the loop, it joins the words in each sentence with a space and processes them using SpaCy's nlp pipeline.\n\nIt then retrieves the lemma of each token in the sentence token.lemma_ \nif its part-of-speech token.pos_ \n","metadata":{}},{"cell_type":"code","source":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[1])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\nnlp = spacy.load('en', disable=['parser', 'ner']):  By default, when loading the English model in SpaCy, both the tagger and the named entity recognizer components are enabled.but here we load the SpaCy English model with only the tagger component enabled. \n\nThe allowed_postags argument allows to specify the part-of-speech tags for which lemmatization should be applied. In this case, the allowed tags are 'NOUN', 'ADJ', 'VERB', and 'ADV', which correspond to nouns, adjectives, verbs, and adverbs, respectively.\n\nWhen the lemmatization() function is called, it will iterate over the tokens in each sentence and check their part-of-speech tags. Only the tokens with allowed tags will be lemmatized, and the rest will be excluded.\n","metadata":{}},{"cell_type":"code","source":"print(data_lemmatized[:1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Create Dictionary,Corpus and Document Frequency","metadata":{}},{"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"id2word is a Dictionary object that maps words to unique IDs created using the corpora.Dictionary class from Gensim.\n\nThe doc2bow() method performs the following steps:\n\nTokenization: It tokenizes the input text, splitting it into individual words or tokens.\n\nWord ID mapping: For each token in the text, it checks if the token exists in the dictionary (id2word). If the token is present, it retrieves the corresponding unique ID assigned to that word. If the token is not present, it assigns a new unique ID to the token and adds it to the dictionary.\n\nCounting word occurrences: It counts the frequency of each word in the document.\n\nOutput: The method returns a list of (word_id, word_frequency) tuples, where word_id is the unique ID of the word in the dictionary, and word_frequency is the count of occurrences of that word in the document.\n\nThe resulting list of tuples represents the bag-of-words representation of the document, where each word is represented by its ID and frequency in the document.","metadata":{}},{"cell_type":"markdown","source":"# Human readable format of corpus (term-frequency)","metadata":{}},{"cell_type":"code","source":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2 : Working with clean_comm_use.csv","metadata":{}},{"cell_type":"code","source":"df1 = clean_comm_use\ndf1 = df1.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# creating n-gram and fetching to HashingVectorizer to get feature vector X","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/maksimeren/covid-19-literature-clustering\nwords = []\nfor ii in range(0,len(df1)):\n    words.append(str(df1.iloc[ii]['text']).split(\" \"))    \n    \nn_gram_all = []\n\nfor word in words:\n    # get n-grams for the instance\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--> for i in range(len(word)-2+1): inner loop that generates n-grams for the current word. It iterates over the range from 0 to the length of the word minus 2 plus 1. This range allows for generating n-grams of length 2.\n\n--> n_gram.append(\"\".join(word[i:i+2])): For each iteration of the inner loop, it retrieves a substring of length 2 from the current word and joins them together to form an n-gram. The resulting n-gram is added to the n_gram list.\n\nAfter generating all the n-grams for the current word, the n_gram list is added to the n_gram_all list which contains the generated n-grams for each word in the text data. The n-grams are formed by concatenating adjacent pairs of words within each instance.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\n\n# hash vectorizer instance\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n\n# features matrix X\nX = hvec.fit_transform(n_gram_all)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"first HashingVectorizer class will be imported. It is then used to create an instance of the HashingVectorizer class.\nthis class provides functionality for converting text data into a fixed-size vector representation using the hashing trick.\n\nlowercase=False --> This parameter indicates that the text should not be converted to lowercase. By default, HashingVectorizer converts text to lowercase.\n\nanalyzer=lambda l:l --> lambda function that simply returns the input as it is, without any further preprocessing or tokenization, that is to ensure that the input n-grams are treated as individual tokens.\n\nn_features=2**12 --> This parameter sets the number of features or dimensions in the resulting vector representation, here 2^12 (4096) features are used. The choice of the number of features depends on the specific application and available computational resources.","metadata":{}},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# test set size of 20% of the data and the random seed 42 <3\nX_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**K-means clustering for 15 cluster**","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nk = 15 \nkmeans = KMeans(n_clusters=k, n_jobs=4, verbose= k)\ny_pred = kmeans.fit_predict(X_train)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"n_jobs=4: This parameter specifies the number of parallel jobs to run for the computation. It is set to 4, indicating that the computation can be distributed across 4 CPU cores if available.\n\nverbose=k: This parameter controls the verbosity of the algorithm. It determines the amount of output text during the clustering process. In this case, the value of k is used, so the algorithm will provide more detailed output.","metadata":{}},{"cell_type":"code","source":"y_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_pred\ny_test = kmeans.predict(X_test)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1)\nX_embedded = tsne.fit_transform(X_train)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(10,10)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n\nplt.title(\"t-SNE Covid-19 Articles\")\n# plt.savefig(\"plots/t-sne_covid19.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing clusters","metadata":{}},{"cell_type":"code","source":"X_embedded[:,1].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(10,10)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered\")\n# plt.savefig(\"plots/t-sne_covid19_label.png\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3 : working with clean_noncomm_use.csv","metadata":{}},{"cell_type":"code","source":"type(clean_noncomm_use.abstract.dropna().tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"gensim.summarization offers TextRank summarization","metadata":{}},{"cell_type":"code","source":"from gensim.summarization.summarizer import summarize\nsummarize(clean_noncomm_use.abstract.dropna().to_string())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Understanding the Text Rank Algorithm","metadata":{}},{"cell_type":"markdown","source":"# TextRank Algorithm","metadata":{}},{"cell_type":"markdown","source":"**We will apply the TextRank algorithm on this dataset of  articles with the aim of creating a concise summary.**","metadata":{}},{"cell_type":"markdown","source":"Now the next step is to break the text into individual sentences. We will use the sent_tokenize( ) function of the nltk library to do this.","metadata":{}},{"cell_type":"code","source":"#ref : https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in clean_noncomm_use.abstract.dropna():\n    sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x] # flatten list","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4 : Analyzing clean_pmc.csv file","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers\n!pip install -U torch","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport json\nfrom transformers import T5Tokenizer, T5ConditionalGeneration, T5Config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarization Task using T5 model","metadata":{}},{"cell_type":"code","source":"df1 = biorxiv_clean\ndf1 = df1.abstract.dropna()\ndf1abstracts = df.values.tolist()\n\nlen(df1abstracts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"T5_PATH = 't5-base'\nt5_model = T5ForConditionalGeneration.from_pretrained(T5_PATH, output_past=True)\nt5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef t5_summarize(input_text, num_beams=4, num_words=80):\n    #input_text = str(input_text).replace('\\n', '')\n    input_text = ' '.join(input_text.split())\n    input_tokenized = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    summary_task = torch.tensor([[21603, 10]]).to(device)\n    input_tokenized = torch.cat([summary_task, input_tokenized], dim=-1).to(device)\n    summary_ids = t5_model.generate(input_tokenized,\n                                    num_beams=int(num_beams),\n                                    no_repeat_ngram_size=3,\n                                    length_penalty=2.0,\n                                    min_length=30,\n                                    max_length=int(num_words),\n                                    early_stopping=True)\n    output = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) \n              for g in summary_ids]\n    return output[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor i in range(20):\n    try:\n        print('BioArvix paper  ',i + 1, \" : \\n\" )\n        print(t5_summarize(df1abstracts[i]))\n        print('............................................................................\\n\\n\\n\\n')\n    except:\n        print('paper ',i+1 ,\" has LONG ABSTRACT\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **evaluation metric** #","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge-score\n!pip install rouge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n\n# Define the generated summary and reference summary\ngenerated_summary = '''lowering the age threshold for quarantine to 50 years of age ICU \nadmissions drastically.similer results are expected to hold for other countries, though \nsome minor adaption will be required'''\nreference_summary = '''Lowering the age threshold for quarantine to 50 years of age could \nlead to a significant reduction in ICU admissions. Similar findings are anticipated for other\ncountries,although slight modifications may be necessary.'''\n\n# Initialize Rouge scorer\nrouge_scorer = Rouge()\n\n# Calculate ROUGE scores\nrouge_scores = rouge_scorer.get_scores(generated_summary, reference_summary)\n\n# Print ROUGE scores\nprint(\"ROUGE Scores:\")\nprint(rouge_scores)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}